# DecissionTrees-CART

Jupyter notebooks here gives the understanding on how decision trees are used both for regression and classification.

Decision trees are resistent to outliers but they are very prone to overfitting - to minimize that use the concept of pruning - cutting the branches/features which have less importance. Basically ensure that tree is small ie., less in depth. 

Select the feature for splitting with high information gain(without having same number of positives and negatives in 'yes' and 'no' branches).

Performance Evaluation:

Regression - I have used R-squared.

Classification - I have used Accuracy.
